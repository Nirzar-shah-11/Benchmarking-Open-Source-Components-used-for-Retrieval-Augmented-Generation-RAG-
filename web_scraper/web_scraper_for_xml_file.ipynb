{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCOB-MWA-8OE",
        "outputId": "a9557d31-e08f-4ba4-e4ab-0505ef87ceb2"
      },
      "outputs": [],
      "source": [
        "!pip install requests beautifulsoup4 fpdf html2text weasyprint reportlab lxml -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "chXogL_TAlZb",
        "outputId": "a615fcd6-3e50-4575-fe7f-ced74880449c"
      },
      "outputs": [],
      "source": [
        "# Webpage Extraction with XML\n",
        "\n",
        "import os\n",
        "import re\n",
        "import textwrap\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.lib.units import mm\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from reportlab.lib.enums import TA_CENTER, TA_LEFT\n",
        "\n",
        "# -------------------------------------\n",
        "# STEP 1: Extract all links from sitemap or webpage\n",
        "# -------------------------------------\n",
        "def extract_links(base_url):\n",
        "    print(f\"üåê Fetching links from: {base_url}\")\n",
        "    response = requests.get(base_url, timeout=15)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    content_type = response.headers.get(\"Content-Type\", \"\")\n",
        "    base_domain = urlparse(base_url).netloc\n",
        "    all_links = set()\n",
        "\n",
        "    # ---- Case 1: XML Sitemap ----\n",
        "    if \"xml\" in content_type or base_url.endswith(\".xml\"):\n",
        "        soup = BeautifulSoup(response.text, \"xml\")\n",
        "        for loc in soup.find_all(\"loc\"):\n",
        "            url = loc.text.strip()\n",
        "            if base_domain in urlparse(url).netloc:\n",
        "                all_links.add(url)\n",
        "        print(f\"‚úÖ Found {len(all_links)} links in XML sitemap.\")\n",
        "\n",
        "    # ---- Case 2: Regular HTML page ----\n",
        "    else:\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        for a_tag in soup.find_all(\"a\", href=True):\n",
        "            href = a_tag[\"href\"]\n",
        "            full_url = urljoin(base_url, href)\n",
        "            if base_domain in urlparse(full_url).netloc:\n",
        "                all_links.add(full_url.split(\"#\")[0])\n",
        "        print(f\"‚úÖ Found {len(all_links)} HTML links on page.\")\n",
        "\n",
        "    return sorted(all_links)\n",
        "\n",
        "# -------------------------------------\n",
        "# STEP 2: Convert webpage ‚Üí Clean PDF\n",
        "# -------------------------------------\n",
        "def create_clean_pdf(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=20)\n",
        "        response.raise_for_status()\n",
        "        html = response.text\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to fetch {url}: {e}\")\n",
        "        return\n",
        "\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "    title = soup.title.string.strip() if soup.title else \"Webpage Content\"\n",
        "    main_content = soup.find(\"main\") or soup.find(\"article\") or soup.body or soup\n",
        "\n",
        "    # Remove unwanted tags\n",
        "    for tag in main_content.find_all([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"noscript\"]):\n",
        "        tag.decompose()\n",
        "\n",
        "    text_parts = []\n",
        "    for elem in main_content.find_all([\"h1\", \"h2\", \"h3\", \"p\", \"li\", \"span\"]):\n",
        "        txt = elem.get_text(strip=True)\n",
        "        if txt:\n",
        "            text_parts.append(txt)\n",
        "\n",
        "    clean_text = \"\\n\\n\".join(text_parts)\n",
        "\n",
        "    # ---------- Save as PDF ----------\n",
        "    os.makedirs(\"webpage_pdfs\", exist_ok=True)\n",
        "    name = re.sub(r\"[^A-Za-z0-9_-]\", \"_\", url.split(\"/\")[-1] or \"index\")\n",
        "    pdf_path = os.path.join(\"webpage_pdfs\", f\"{name}.pdf\")\n",
        "\n",
        "    doc = SimpleDocTemplate(pdf_path, pagesize=A4,\n",
        "                            rightMargin=20*mm, leftMargin=20*mm,\n",
        "                            topMargin=20*mm, bottomMargin=20*mm)\n",
        "\n",
        "    styles = getSampleStyleSheet()\n",
        "    styles.add(ParagraphStyle(name='CenterTitle', alignment=TA_CENTER,\n",
        "                              fontSize=16, leading=20, spaceAfter=10))\n",
        "    styles.add(ParagraphStyle(name='Body', alignment=TA_LEFT,\n",
        "                              fontSize=10, leading=12))\n",
        "\n",
        "    flow = []\n",
        "    flow.append(Paragraph(f\"{title} ‚Äî Hof University\", styles['CenterTitle']))\n",
        "    flow.append(Paragraph(f\"Source: {url}\", styles['Body']))\n",
        "    flow.append(Spacer(1, 8))\n",
        "\n",
        "    for para in clean_text.split(\"\\n\\n\"):\n",
        "        wrapped = \"\\n\".join(textwrap.fill(line, 95) for line in para.splitlines())\n",
        "        flow.append(Paragraph(wrapped.replace(\"\\n\", \"<br/>\"), styles['Body']))\n",
        "        flow.append(Spacer(1, 4))\n",
        "\n",
        "    doc.build(flow)\n",
        "    print(f\"üìÑ PDF created: {pdf_path}\")\n",
        "\n",
        "# -------------------------------------\n",
        "# STEP 3: Run the whole pipeline\n",
        "# -------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    base_url = \" Provide xml file of the website here \"\n",
        "\n",
        "    # 1. Extract all subpage links\n",
        "    links = extract_links(base_url)\n",
        "\n",
        "    # 2. Save to file (optional)\n",
        "    with open(\"urls.txt\", \"w\") as f:\n",
        "        for link in links:\n",
        "            f.write(link + \"\\n\")\n",
        "    print(\"üìù Saved all links to urls.txt\")\n",
        "\n",
        "    # 3. Convert all links to PDFs\n",
        "    for i, link in enumerate(links, 1):\n",
        "        print(f\"\\n[{i}/{len(links)}] Processing: {link}\")\n",
        "        create_clean_pdf(link)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
