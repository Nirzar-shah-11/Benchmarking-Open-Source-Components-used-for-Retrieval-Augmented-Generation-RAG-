{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mguBujKiyWLv"
   },
   "outputs": [],
   "source": [
    "!pip install -q chromadb sentence-transformers PyPDF2 langchain langchain-community langchain-huggingface langchain-text-splitters accelerate ragas datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQ0K2-2f_squ",
    "outputId": "bfcd3691-3fd4-402f-f5f1-f7d320748529"
   },
   "outputs": [],
   "source": [
    "# Step 1: Imports\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import re\n",
    "import json\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from PyPDF2 import PdfReader\n",
    "import time\n",
    "!rm -rf course_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "#  Enter huggingface token here\n",
    "login(token=\"*****\")\n",
    "\n",
    "# Select and one embedding model and comment out the rest\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "embedding_model = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "embedding_model = \"sentence-transformers/distiluse-base-multilingual-cased-v2\"\n",
    "embedding_model = \"intfloat/multilingual-e5-large\"\n",
    "# Embedding model\n",
    "model = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "\n",
    "persist_dir = \"./course_db\"\n",
    "os.makedirs(persist_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gIWT_lxL_xk-"
   },
   "outputs": [],
   "source": [
    "# Step 2: Define PDF files and course mapping\n",
    "# Update the path according to you current folder structure \n",
    "course_files = {\n",
    "    \"Master in AI-Driven Supply Chain Management\": \"Course_list/ai-driven-supply-chain-management-msc_html.pdf\",\n",
    "    \"Master in Applied Psychology\": \"Course_list/applied-psychology-msc_html.pdf\",\n",
    "    \"Master in Applied Research in Computer Science\": \"Course_list/applied-research-in-computer-science-msc_html.pdf\",\n",
    "    \"Master in Artificial Intelligence Aided Mobility Design\": \"Course_list/artificial-intelligence-aided-mobility-design-ma_html.pdf\",\n",
    "    \"Master in Artificial Intelligence and Robotics\": \"Course_list/artificial-intelligence-and-robotics-msc_html.pdf\",\n",
    "    \"Bachlors in Business Administration\": \"Course_list/business-administration-ba_html.pdf\",\n",
    "    \"Bachlors in Business Information Systems\": \"Course_list/business-information-systems-bsc_html.pdf\",\n",
    "    \"Bachlors in Business Law\": \"Course_list/business-law-llb_html.pdf\",\n",
    "    \"Bachlors in Communication Design\": \"Course_list/communication-design-ba_html.pdf\",\n",
    "    \"Master in Compliance IT and Data Protection (LLM)\": \"Course_list/compliance-it-and-data-protection-llm_html.pdf\",\n",
    "    \"Master in Compliance IT and Data Protection (MBA)\": \"Course_list/compliance-it-and-data-protection-mba_html.pdf\",\n",
    "    \"Master in Composite Materials\": \"Course_list/composite-materials-meng_html.pdf\",\n",
    "    \"Bachlors in Computer Science\": \"Course_list/computer-science-bsc_html.pdf\",\n",
    "    \"Bachlors in Computer Science International\": \"Course_list/computer-science-international-bsc_html.pdf\",\n",
    "    \"Master in Computer Science\": \"Course_list/computer-science-msc_html.pdf\",\n",
    "    \"Master in Cross-Cultural Nursing Practice\": \"Course_list/cross-cultural-nursing-practice-msc_html.pdf\",\n",
    "    \"Bachlors in Design and Mobility\": \"Course_list/design-and-mobility-ba_html.pdf\",\n",
    "    \"Master in Digital Business Management\": \"Course_list/digital-business-management-msc_html.pdf\",\n",
    "    \"Master in Digitalization and Innovation\": \"Course_list/digitalization-and-innovation-mba_html.pdf\",\n",
    "    \"Bachlors in Economic and Organizational Sociology\": \"Course_list/economic-and-organizational-sociology-ba_html.pdf\",\n",
    "    \"Bachlors in Economic Psychology\": \"Course_list/economic-psychology-bsc_html.pdf\",\n",
    "    \"Bachlors in Engineering Science International\": \"Course_list/engineering-science-international-beng_html.pdf\",\n",
    "    \"Bachlors in Engineering Sciences\": \"Course_list/engineering-sciences-beng.html.pdf\",\n",
    "    \"Bachlors in Environmental Engineering\": \"Course_list/environmental-engineering-beng.html.pdf\",\n",
    "    \"Master in General Management\": \"Course_list/general-management-mba_html.pdf\",\n",
    "    \"Master in Global Management\": \"Course_list/global-management-ma_html.pdf\",\n",
    "    \"Master in Human Resources and Organizational Management\": \"Course_list/human-resources-and-organizational-management_html.pdf\",\n",
    "    \"Master in Information Systems in Public Administration\": \"Course_list/information-systems-in-public-administration-msc_html.pdf\",\n",
    "    \"Bachlors in Innovative Healthcare\": \"Course_list/innovative-healthcare-bsc_html.pdf\",\n",
    "    \"Bachlors in Innovative Textiles\": \"Course_list/innovative-textiles-beng_html.pdf\",\n",
    "    \"Bachlors in International Management\": \"Course_list/international-management-ba_html.pdf\",\n",
    "    \"Master in International Project Management\": \"Course_list/international-project-management-ma_html.pdf\",\n",
    "    \"Master in Marketing Management\": \"Course_list/marketing-management-msc_html.pdf\",\n",
    "    \"Master in Mechanical Engineering\": \"Course_list/mechanical-engineering-meng.html.pdf\",\n",
    "    \"Bachlors in Media Informatics\": \"Course_list/media-informatics-bsc_html.pdf\",\n",
    "    \"Bachlors in Mobile App Development\": \"Course_list/mobile-app-development-bsc_html.pdf\",\n",
    "    \"Bachlors in Nursing (Nursing Profession Act)\": \"Course_list/nursing-bsc-according-to-the-nursing-profession-act_html.pdf\",\n",
    "    \"Bachlors in Nursing (Experienced Professionals)\": \"Course_list/nursing-bsc-for-experienced-professionals_html.pdf\",\n",
    "    \"Master in Operational Excellence\": \"Course_list/operational-excellence-mba-and-eng_html.pdf\",\n",
    "    \"Master in Smart Society\": \"Course_list/smart-society-msc_html.pdf\",\n",
    "    \"Master in Software Engineering for Industrial Applications\": \"Course_list/software-engineering-for-industrial-applications-meng.html.pdf\",\n",
    "    \"Master in Supply Chain Management\": \"Course_list/supply-chain-management-msc_html.pdf\",\n",
    "    \"Master in Sustainable Engineering and Project Management\": \"Course_list/sustainable-engineering-and-project-management-mba-and-eng_html.pdf\",\n",
    "    \"Master in Sustainable Textiles\": \"Course_list/sustainable-textiles-meng_html.pdf\",\n",
    "    \"Master in Sustainable Water Management and Engineering\": \"Course_list/sustainable-water-management-and-engineering-meng_html.pdf\",\n",
    "    \"Bachlors in Textile Design\": \"Course_list/textile-design-ba_html.pdf\",\n",
    "    \"Master in Textile Design\": \"Course_list/textile-design-ma_html.pdf\",\n",
    "    \"Bachlors in Vocational Education in Health Care\": \"Course_list/vocational-education-in-health-care-ba.html.pdf\"\n",
    "}\n",
    "course_names = list(course_files.keys())\n",
    "course_embs = model.embed_documents(course_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlzkMJAa_zkk"
   },
   "outputs": [],
   "source": [
    "# Step 3: Define function for chunking\n",
    "\n",
    "def read_pdf_chunks(filepath, chunk_size=700, overlap=200):\n",
    "    \"\"\"\n",
    "    Reads text from a PDF and splits it into semantically meaningful chunks.\n",
    "    This uses LangChain's RecursiveCharacterTextSplitter, which respects sentence and paragraph boundaries.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(filepath)\n",
    "    full_text = \"\"\n",
    "\n",
    "    # Combine text from all pages\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text() or \"\"\n",
    "        full_text += page_text + \"\\n\"\n",
    "\n",
    "    # Use RecursiveCharacterTextSplitter for semantic chunking\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \".\", \"!\", \"?\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_text(full_text)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZWJPc-uxG_mS",
    "outputId": "313980b4-f9b0-4113-d136-7510723b74b3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Build Chroma Database with metadata for all courses\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"course_docs\",\n",
    "    persist_directory=persist_dir,\n",
    "    embedding_function=model\n",
    ")\n",
    "\n",
    "def add_pdf_to_chromadb(course_name, pdf_path, total_indexing_time):\n",
    "  \"\"\"Add PDF text chunks and metadata to Chroma collection.\"\"\"\n",
    "  start_indexing_time = time.time()\n",
    "  chunks = read_pdf_chunks(pdf_path)\n",
    "\n",
    "\n",
    "  ids = [f\"{course_name}_{i}\" for i in range(len(chunks))]\n",
    "  metadatas = [{\"course\": course_name, \"filename\": pdf_path}] * len(chunks)\n",
    "\n",
    "  vector_store.add_texts(\n",
    "      texts=chunks,\n",
    "      metadatas=metadatas,\n",
    "      ids=ids\n",
    "  )\n",
    "\n",
    "  end_indexing_time = time.time()\n",
    "  total_indexing_time += (end_indexing_time - start_indexing_time)\n",
    "\n",
    "  print(f\"Added {len(chunks)} chunks for '{course_name}'\")\n",
    "  return total_indexing_time\n",
    "# Add all PDFs to the ChromaDB collection and measure indexing time\n",
    "\n",
    "total_indexing_time = 0\n",
    "for name, path in course_files.items():\n",
    "    total_indexing_time = add_pdf_to_chromadb(name, path, total_indexing_time)\n",
    "\n",
    "vector_store.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTzA4RrpHLAP"
   },
   "outputs": [],
   "source": [
    "# Step 5: Define function for detecting Course Name and cleaning query\n",
    "\n",
    "def detect_course_and_clean_query(query, min_course_score=0.35, min_span_score=0.25, max_ngram=6):\n",
    "  \"\"\"Detect course name from query and remove its span.\"\"\"\n",
    "  query_emb = model.embed_documents([query])[0]\n",
    "  cos_scores = util.cos_sim(query_emb, course_embs)[0]\n",
    "\n",
    "  best_idx = int(cos_scores.argmax().item())\n",
    "  best_course = course_names[best_idx]\n",
    "  best_course_score = float(cos_scores[best_idx].item())\n",
    "\n",
    "  if best_course_score < min_course_score:\n",
    "      return None, query\n",
    "\n",
    "  # Find matching n-gram for course name\n",
    "  tokens = query.split()\n",
    "  candidates = [\" \".join(tokens[i:i+L]) for L in range(1, max_ngram+1) for i in range(len(tokens)-L+1)]\n",
    "  cand_embs = model.embed_documents(candidates)\n",
    "  cand_scores = util.cos_sim(course_embs[best_idx], cand_embs)[0]\n",
    "\n",
    "  best_cand = candidates[int(cand_scores.argmax().item())]\n",
    "  best_cand_score = float(cand_scores.max().item())\n",
    "\n",
    "  cleaned_query = query\n",
    "  if best_cand_score > min_span_score:\n",
    "      pattern = re.escape(best_cand)\n",
    "      cleaned_query = re.sub(pattern, \"\", query, flags=re.IGNORECASE, count=1).strip()\n",
    "      cleaned_query = re.sub(r'\\b(for|of|about|in|on)\\b\\s*([?.!,;:]?)$', r'\\2', cleaned_query, flags=re.IGNORECASE).strip()\n",
    "\n",
    "  return best_course, cleaned_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EyyoSdqRHSuw"
   },
   "outputs": [],
   "source": [
    "# Step 6: Retrieve from the detected course index\n",
    "\n",
    "# Helper: build filtered retriever like FAISS version\n",
    "def get_filtered_retriever(course_name=None, k=5):\n",
    "    if course_name:\n",
    "        return vector_store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": k, \"filter\": {\"course\": course_name}}\n",
    "        )\n",
    "    else:\n",
    "        return vector_store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": k}\n",
    "        )\n",
    "\n",
    "def retrieve(query, k=3):\n",
    "    # Load your persisted Chroma collection using LangChain wrapper\n",
    "    vector_store = Chroma(\n",
    "        collection_name=\"course_docs\",\n",
    "        persist_directory=\"./course_db\",\n",
    "        embedding_function=model\n",
    "    )\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Course detection logic\n",
    "    course, clean_query = detect_course_and_clean_query(query)\n",
    "\n",
    "    if course:\n",
    "        print(f\"ðŸŽ¯ Matched course (by similarity): {course}\")\n",
    "        retriever = get_filtered_retriever(course, k)\n",
    "        docs = retriever.invoke(clean_query)\n",
    "    else:\n",
    "        print(\"âš ï¸ No strong course match â€” performing global search.\")\n",
    "        retriever = get_filtered_retriever(None, k)\n",
    "        docs = retriever.invoke(clean_query)\n",
    "        \n",
    "    results = [(doc.metadata.get(\"source\", 'unknown'), doc.page_content) for doc in docs]\n",
    "    retrieval_time = time.time() - start_time\n",
    "\n",
    "\n",
    "    print(f\"â±ï¸ Retrieval time: {time.time() - start_time:.2f}s â€” {len(results)} chunks returned.\")\n",
    "    return results, retrieval_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u9I-SqOCHZjy",
    "outputId": "1875dbf9-4bcd-4e54-a8ac-882f50e9c8f0"
   },
   "outputs": [],
   "source": [
    "# Step 7: Test the Retriver\n",
    "\n",
    "query = \"what are Language requirements for the course AI-Driven Supply Chain Management?\"\n",
    "results, retrieval_time  = retrieve(query, k=3)\n",
    "\n",
    "for i, (course, text) in enumerate(results, 1):\n",
    "    print(f\"\\n--- Result {i} (Course: {course}) ---\\n{text[:800]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TDEvBpoiWVw"
   },
   "outputs": [],
   "source": [
    "# Step 8: Create a prompt template\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert assistant. Answer the question using ONLY the provided context.\n",
    "If the answer is not in the context, say you don't know.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "868719aa166f4e7080debcd78fa455d2",
      "a474d28a88074202988a5490f46e075d",
      "5b4f9e1ca0274b4a9f89dac6a3ed715a",
      "a459272fb0c74fb19708f6dcaf6f6f7f",
      "f5784ecd9935440bbc9c5dc6d0abe4de",
      "e92d0a04e67840419e07dc0f12d3eff6",
      "7f333f31deac4ea5921b587d916cdc06",
      "fd3419cce89748e98201343d19951b9f",
      "50d7f257954643e7b14762e2167c3844",
      "875fea4b974d4df599c1c37e1c6d6060",
      "1b10dd68aa054137a790d61c300ecd45"
     ]
    },
    "id": "O27KiJ24iWM8",
    "outputId": "b12d0aba-bfb6-43ff-daca-0b9b43cc7d4a"
   },
   "outputs": [],
   "source": [
    "# Step 9: Define LLM\n",
    "\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "\n",
    "# 1. model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# 2. model_id = \"mistralai/Ministral-8B-Instruct-2410\"\n",
    "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map = \"auto\",\n",
    "    dtype = \"auto\",\n",
    ")\n",
    "\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=llm_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "\n",
    "# Create HuggingFacePipeline instance\n",
    "hf_llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "# Pass the HuggingFacePipeline instance as the 'llm' argument to ChatHuggingFace\n",
    "llm = ChatHuggingFace(llm=hf_llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpB0wS1QiWA8"
   },
   "outputs": [],
   "source": [
    "# Step 10: Build a langchain\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "rag_chain = (RunnableParallel({\n",
    "    \"context\" : lambda x: retrieve(x[\"question\"], k=3),\n",
    "    \"question\" : lambda x: x[\"question\"]\n",
    "}))|prompt|llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "k3Bz2N5kifaf",
    "outputId": "627c7585-2602-4430-aa5a-65b546bd73bc"
   },
   "outputs": [],
   "source": [
    "# Step 11: Test the chain\n",
    "response = rag_chain.invoke({\"question\": \"Give me Program objectives for ai in supply chain?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_relevant(chunk_text, ground_truth, threshold=0.6):\n",
    "    \"\"\"\n",
    "    Returns True if retrieved chunk is semantically similar\n",
    "    to the ground-truth answer.\n",
    "    \"\"\"\n",
    "    emb_chunk = model.embed_documents([chunk_text])[0]\n",
    "    emb_gt = model.embed_documents([ground_truth])[0]\n",
    "    score = util.cos_sim(emb_chunk, emb_gt).item()\n",
    "    return score >= threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr_semantic(retrieved_texts, ground_truth, threshold=0.6):\n",
    "    for rank, text in enumerate(retrieved_texts, start=1):\n",
    "        if is_relevant(text, ground_truth, threshold):\n",
    "            return 1.0 / rank\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "with open(\"raga_eval_dataset_english.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "records = []\n",
    "\n",
    "for sample in tqdm(eval_data):\n",
    "    query = sample[\"query\"]\n",
    "    ground_truth = sample[\"ground_truth\"]\n",
    "\n",
    "    results, retrieval_time = retrieve(query, k=5)\n",
    "\n",
    "    retrieved_texts = [text for _, text in results]\n",
    "\n",
    "    mrr = mrr_semantic(retrieved_texts, ground_truth)\n",
    "\n",
    "    records.append({\n",
    "        \"query\": query,\n",
    "        \"MRR\": mrr,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "Mean_Recall_Rate = {\n",
    "    \"MRR\": df[\"MRR\"].mean()\n",
    "}\n",
    "\n",
    "print(Mean_Recall_Rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "AecKTu5Ydiod",
    "outputId": "14b22742-54ba-45bf-e2e9-f92e299c7e1f"
   },
   "outputs": [],
   "source": [
    "# Step 12: Import the RAGAS\n",
    "\n",
    "from ragas import evaluate\n",
    "import json\n",
    "from ragas.metrics import (\n",
    "    ContextPrecision,\n",
    "    ContextRecall,\n",
    "    Faithfulness,\n",
    "    ResponseGroundedness,\n",
    "    AnswerAccuracy,\n",
    "    SemanticSimilarity\n",
    ")\n",
    "\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "CL599_Z2NiQO"
   },
   "outputs": [],
   "source": [
    "# Step 13: Define function to execute the query without using langchain\n",
    "def run_rag(question, k=3):\n",
    "    results, retrieval_time = retrieve(question, k)\n",
    "    contexts = [text for _, text in results]\n",
    "\n",
    "    prompt_text = prompt.format(\n",
    "        question=question,\n",
    "        context=\"\\n\\n\".join(contexts)\n",
    "    )\n",
    "\n",
    "    llm_start = time.time()\n",
    "    response = llm.invoke(prompt_text)\n",
    "    llm_time = time.time() - llm_start\n",
    "\n",
    "    return {\n",
    "        \"answer\": response.content,\n",
    "        \"contexts\": contexts,\n",
    "        \"retrieval_time\": retrieval_time,\n",
    "        \"llm_time\": llm_time,\n",
    "        \"total_time\": retrieval_time + llm_time\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 14a: Import the Evaluation data and evaluate it\n",
    "\n",
    "with open(\"raga_eval_dataset_english.json\", \"r\") as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "ragas_records = []\n",
    "total_retrieval_time = 0\n",
    "total_generation_time = 0\n",
    "\n",
    "for item in eval_data:\n",
    "    result = run_rag(item[\"query\"], k=3)\n",
    "    total_retrieval_time = total_retrieval_time + result[\"retrieval_time\"]\n",
    "    total_generation_time = total_generation_time + result[\"llm_time\"]\n",
    "\n",
    "    answer = result[\"answer\"]\n",
    "    contexts = result[\"contexts\"]\n",
    "\n",
    "    ragas_records.append({\n",
    "        \"question\": item[\"query\"],\n",
    "        \"answer\": answer,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truth\": item[\"ground_truth\"]\n",
    "    })\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "ragas_dataset = Dataset.from_list(ragas_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 14b: Import the Evaluation data and evaluate it\n",
    "\n",
    "with open(\"raga_eval_dataset_english.json\", \"r\") as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "ragas_records_sematic = []\n",
    "\n",
    "for item in eval_data:\n",
    "    result = run_rag(item[\"query\"], k=3)\n",
    "    \n",
    "    answer = result[\"answer\"]\n",
    "    contexts = result[\"contexts\"]\n",
    "\n",
    "    ragas_records_sematic.append({\n",
    "        \"question\": item[\"query\"],\n",
    "        \"answer\": answer,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truth\": item[\"ground_truth\"]\n",
    "    })\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "ragas_records_sematic = Dataset.from_list(ragas_records_sematic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Enter Open AI Api key here\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"***\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 15a: Evaluate the RAG\n",
    "\n",
    "from ragas.run_config import RunConfig \n",
    "results1 = evaluate(\n",
    "    dataset=ragas_dataset,\n",
    "    metrics=[\n",
    "        ContextPrecision(),\n",
    "        ContextRecall(),\n",
    "        Faithfulness(),\n",
    "    ],\n",
    "    embeddings=model,\n",
    "    run_config=RunConfig(max_workers=1)\n",
    ")\n",
    "\n",
    "print(results1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 15b: Evaluate the RAG\n",
    "\n",
    "results2 = evaluate(\n",
    "    dataset=ragas_records_sematic,\n",
    "    metrics=[\n",
    "        SemanticSimilarity()\n",
    "    ],\n",
    "    embeddings=model,\n",
    "    llm = llm,\n",
    "    run_config=RunConfig(max_workers=1)\n",
    ")\n",
    "\n",
    "print(results2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"For Embedding model:{embedding_model} and LLM {model_id}\") \n",
    "print(f\"Total Retrieval Time: {total_retrieval_time}\")\n",
    "print(f\"Total Generation Time per query: {total_generation_time}\")\n",
    "print(f\"Total Indexing Time: {total_indexing_time}\")\n",
    "print(f\"Mean Recall Rate: {Mean_Recall_Rate.values()}\")\n",
    "print(\"RAGAS metrics\")\n",
    "print(results1)\n",
    "print(results2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_folder_size(path):\n",
    "    total = 0\n",
    "    for dirpath, _, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total += os.path.getsize(fp)\n",
    "    return total\n",
    "\n",
    "size_bytes = get_folder_size(\"course_db\")\n",
    "print(f\"Size: {size_bytes / (1024**2)} MB\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1b10dd68aa054137a790d61c300ecd45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "50d7f257954643e7b14762e2167c3844": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5b4f9e1ca0274b4a9f89dac6a3ed715a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd3419cce89748e98201343d19951b9f",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_50d7f257954643e7b14762e2167c3844",
      "value": 2
     }
    },
    "7f333f31deac4ea5921b587d916cdc06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "868719aa166f4e7080debcd78fa455d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a474d28a88074202988a5490f46e075d",
       "IPY_MODEL_5b4f9e1ca0274b4a9f89dac6a3ed715a",
       "IPY_MODEL_a459272fb0c74fb19708f6dcaf6f6f7f"
      ],
      "layout": "IPY_MODEL_f5784ecd9935440bbc9c5dc6d0abe4de"
     }
    },
    "875fea4b974d4df599c1c37e1c6d6060": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a459272fb0c74fb19708f6dcaf6f6f7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_875fea4b974d4df599c1c37e1c6d6060",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1b10dd68aa054137a790d61c300ecd45",
      "value": "â€‡2/2â€‡[00:00&lt;00:00,â€‡20.66it/s]"
     }
    },
    "a474d28a88074202988a5490f46e075d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e92d0a04e67840419e07dc0f12d3eff6",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7f333f31deac4ea5921b587d916cdc06",
      "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
     }
    },
    "e92d0a04e67840419e07dc0f12d3eff6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5784ecd9935440bbc9c5dc6d0abe4de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd3419cce89748e98201343d19951b9f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
