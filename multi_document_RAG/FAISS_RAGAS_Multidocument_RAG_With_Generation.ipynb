{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers PyPDF2 langchain langchain-community langchain-huggingface langchain-text-splitters accelerate faiss-cpu ragas datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dh4RU0kyPUXh"
   },
   "outputs": [],
   "source": [
    "# Step 1: Imports\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "#  Enter huggingface token here\n",
    "login(token=\"*****\")\n",
    "\n",
    "# Select and one embedding model and comment out the rest\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "embedding_model = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "embedding_model = \"sentence-transformers/distiluse-base-multilingual-cased-v2\"\n",
    "embedding_model = \"intfloat/multilingual-e5-large\"\n",
    "# Embedding model\n",
    "model = HuggingFaceEmbeddings(model_name=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf faiss_course_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vT8wJez7olXC"
   },
   "outputs": [],
   "source": [
    "# Step 2: Define PDF files and course mapping\n",
    "# Update the path according to you current folder structure \n",
    "course_files = {\n",
    "    \"Master in AI-Driven Supply Chain Management\": \"Course_list/ai-driven-supply-chain-management-msc_html.pdf\",\n",
    "    \"Master in Applied Psychology\": \"Course_list/applied-psychology-msc_html.pdf\",\n",
    "    \"Master in Applied Research in Computer Science\": \"Course_list/applied-research-in-computer-science-msc_html.pdf\",\n",
    "    \"Master in Artificial Intelligence Aided Mobility Design\": \"Course_list/artificial-intelligence-aided-mobility-design-ma_html.pdf\",\n",
    "    \"Master in Artificial Intelligence and Robotics\": \"Course_list/artificial-intelligence-and-robotics-msc_html.pdf\",\n",
    "    \"Bachlors in Business Administration\": \"Course_list/business-administration-ba_html.pdf\",\n",
    "    \"Bachlors in Business Information Systems\": \"Course_list/business-information-systems-bsc_html.pdf\",\n",
    "    \"Bachlors in Business Law\": \"Course_list/business-law-llb_html.pdf\",\n",
    "    \"Bachlors in Communication Design\": \"Course_list/communication-design-ba_html.pdf\",\n",
    "    \"Master in Compliance IT and Data Protection (LLM)\": \"Course_list/compliance-it-and-data-protection-llm_html.pdf\",\n",
    "    \"Master in Compliance IT and Data Protection (MBA)\": \"Course_list/compliance-it-and-data-protection-mba_html.pdf\",\n",
    "    \"Master in Composite Materials\": \"Course_list/composite-materials-meng_html.pdf\",\n",
    "    \"Bachlors in Computer Science\": \"Course_list/computer-science-bsc_html.pdf\",\n",
    "    \"Bachlors in Computer Science International\": \"Course_list/computer-science-international-bsc_html.pdf\",\n",
    "    \"Master in Computer Science\": \"Course_list/computer-science-msc_html.pdf\",\n",
    "    \"Master in Cross-Cultural Nursing Practice\": \"Course_list/cross-cultural-nursing-practice-msc_html.pdf\",\n",
    "    \"Bachlors in Design and Mobility\": \"Course_list/design-and-mobility-ba_html.pdf\",\n",
    "    \"Master in Digital Business Management\": \"Course_list/digital-business-management-msc_html.pdf\",\n",
    "    \"Master in Digitalization and Innovation\": \"Course_list/digitalization-and-innovation-mba_html.pdf\",\n",
    "    \"Bachlors in Economic and Organizational Sociology\": \"Course_list/economic-and-organizational-sociology-ba_html.pdf\",\n",
    "    \"Bachlors in Economic Psychology\": \"Course_list/economic-psychology-bsc_html.pdf\",\n",
    "    \"Bachlors in Engineering Science International\": \"Course_list/engineering-science-international-beng_html.pdf\",\n",
    "    \"Bachlors in Engineering Sciences\": \"Course_list/engineering-sciences-beng.html.pdf\",\n",
    "    \"Bachlors in Environmental Engineering\": \"Course_list/environmental-engineering-beng.html.pdf\",\n",
    "    \"Master in General Management\": \"Course_list/general-management-mba_html.pdf\",\n",
    "    \"Master in Global Management\": \"Course_list/global-management-ma_html.pdf\",\n",
    "    \"Master in Human Resources and Organizational Management\": \"Course_list/human-resources-and-organizational-management_html.pdf\",\n",
    "    \"Master in Information Systems in Public Administration\": \"Course_list/information-systems-in-public-administration-msc_html.pdf\",\n",
    "    \"Bachlors in Innovative Healthcare\": \"Course_list/innovative-healthcare-bsc_html.pdf\",\n",
    "    \"Bachlors in Innovative Textiles\": \"Course_list/innovative-textiles-beng_html.pdf\",\n",
    "    \"Bachlors in International Management\": \"Course_list/international-management-ba_html.pdf\",\n",
    "    \"Master in International Project Management\": \"Course_list/international-project-management-ma_html.pdf\",\n",
    "    \"Master in Marketing Management\": \"Course_list/marketing-management-msc_html.pdf\",\n",
    "    \"Master in Mechanical Engineering\": \"Course_list/mechanical-engineering-meng.html.pdf\",\n",
    "    \"Bachlors in Media Informatics\": \"Course_list/media-informatics-bsc_html.pdf\",\n",
    "    \"Bachlors in Mobile App Development\": \"Course_list/mobile-app-development-bsc_html.pdf\",\n",
    "    \"Bachlors in Nursing (Nursing Profession Act)\": \"Course_list/nursing-bsc-according-to-the-nursing-profession-act_html.pdf\",\n",
    "    \"Bachlors in Nursing (Experienced Professionals)\": \"Course_list/nursing-bsc-for-experienced-professionals_html.pdf\",\n",
    "    \"Master in Operational Excellence\": \"Course_list/operational-excellence-mba-and-eng_html.pdf\",\n",
    "    \"Master in Smart Society\": \"Course_list/smart-society-msc_html.pdf\",\n",
    "    \"Master in Software Engineering for Industrial Applications\": \"Course_list/software-engineering-for-industrial-applications-meng.html.pdf\",\n",
    "    \"Master in Supply Chain Management\": \"Course_list/supply-chain-management-msc_html.pdf\",\n",
    "    \"Master in Sustainable Engineering and Project Management\": \"Course_list/sustainable-engineering-and-project-management-mba-and-eng_html.pdf\",\n",
    "    \"Master in Sustainable Textiles\": \"Course_list/sustainable-textiles-meng_html.pdf\",\n",
    "    \"Master in Sustainable Water Management and Engineering\": \"Course_list/sustainable-water-management-and-engineering-meng_html.pdf\",\n",
    "    \"Bachlors in Textile Design\": \"Course_list/textile-design-ba_html.pdf\",\n",
    "    \"Master in Textile Design\": \"Course_list/textile-design-ma_html.pdf\",\n",
    "    \"Bachlors in Vocational Education in Health Care\": \"Course_list/vocational-education-in-health-care-ba.html.pdf\"\n",
    "}\n",
    "course_names = list(course_files.keys())\n",
    "course_embs = model.embed_documents(course_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4EYM3v-Uk6r"
   },
   "outputs": [],
   "source": [
    "# Step 3: Define function for chunking\n",
    "\n",
    "def read_pdf_chunks(filepath, chunk_size=700, overlap=200):\n",
    "    \"\"\"\n",
    "    Reads text from a PDF and splits it into semantically meaningful chunks.\n",
    "    This uses LangChain's RecursiveCharacterTextSplitter, which respects sentence and paragraph boundaries.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(filepath)\n",
    "    full_text = \"\"\n",
    "\n",
    "    # Combine text from all pages\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text() or \"\"\n",
    "        full_text += page_text + \"\\n\"\n",
    "\n",
    "    # Use RecursiveCharacterTextSplitter for semantic chunking\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \".\", \"!\", \"?\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_text(full_text)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pGNj3cF9Nliw",
    "outputId": "da67cc95-e8f9-4dde-8805-0638575e92a5"
   },
   "outputs": [],
   "source": [
    "# Step 4: Build one FAISS store with metadata for all courses\n",
    "\n",
    "total_indexing_time = 0.0\n",
    "index_build_start = time.time()\n",
    "\n",
    "all_texts, all_metadatas = [], []\n",
    "\n",
    "for course, filepath in course_files.items():\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Missing file for {course}: {filepath}\")\n",
    "        continue\n",
    "    chunks = read_pdf_chunks(filepath)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        all_texts.append(chunk)\n",
    "        all_metadatas.append({\"source\": course, \"chunk_id\": i})\n",
    "        \n",
    "    print(f\"Added {len(chunks)} chunks for {course}\")\n",
    "\n",
    "\n",
    "print(\"Building single FAISS store ...\")\n",
    "vector_store = FAISS.from_texts(all_texts, model, metadatas=all_metadatas)\n",
    "\n",
    "total_indexing_time = time.time() - index_build_start\n",
    "print(f\"FAISS store built in {total_indexing_time:.2f}s with {len(all_texts)} chunks.\")\n",
    "\n",
    "vector_store.save_local(\"faiss_course_db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define function for detecting Course Name and cleaning query\n",
    "\n",
    "def detect_course_and_clean_query(query, min_course_score=0.35, min_span_score=0.25, max_ngram=6):\n",
    "  \"\"\"Detect course name from query and remove its span.\"\"\"\n",
    "  query_emb = model.embed_documents([query])[0]\n",
    "  cos_scores = util.cos_sim(query_emb, course_embs)[0]\n",
    "\n",
    "  best_idx = int(cos_scores.argmax().item())\n",
    "  best_course = course_names[best_idx]\n",
    "  best_course_score = float(cos_scores[best_idx].item())\n",
    "\n",
    "  if best_course_score < min_course_score:\n",
    "      return None, query\n",
    "\n",
    "  # Find matching n-gram for course name\n",
    "  tokens = query.split()\n",
    "  candidates = [\" \".join(tokens[i:i+L]) for L in range(1, max_ngram+1) for i in range(len(tokens)-L+1)]\n",
    "  cand_embs = model.embed_documents(candidates)\n",
    "  cand_scores = util.cos_sim(course_embs[best_idx], cand_embs)[0]\n",
    "\n",
    "\n",
    "  best_cand = candidates[int(cand_scores.argmax().item())]\n",
    "  best_cand_score = float(cand_scores.max().item())\n",
    "\n",
    "  cleaned_query = query\n",
    "  if best_cand_score > min_span_score:\n",
    "      pattern = re.escape(best_cand)\n",
    "      cleaned_query = re.sub(pattern, \"\", query, flags=re.IGNORECASE, count=1).strip()\n",
    "      cleaned_query = re.sub(r'\\b(for|of|about|in|on)\\b\\s*([?.!,;:]?)$', r'\\2', cleaned_query, flags=re.IGNORECASE).strip()\n",
    "\n",
    "  return best_course, cleaned_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cU9ZseekNrkN"
   },
   "outputs": [],
   "source": [
    "# Step 6: Retrieve from the detected course index\n",
    "\n",
    "faiss_vector_store = FAISS.load_local(\n",
    "    \"faiss_course_db\",\n",
    "    embeddings=model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "# Step 6: Retrieve from the detected course index\n",
    "\n",
    "# Helper to create a filtered retriever per course\n",
    "def get_filtered_retriever(course_name=None, k=5):\n",
    "    if course_name:\n",
    "        return vector_store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": k,\"fetch_k\": 1000, \"filter\": {\"source\": course_name}}\n",
    "        )\n",
    "    else:\n",
    "        return vector_store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": k}\n",
    "        )\n",
    "\n",
    "\n",
    "# New retrieve() built on top of the single vector store\n",
    "def retrieve(query, k=3):\n",
    "    start_time = time.time()\n",
    "    course, clean_query = detect_course_and_clean_query(query)\n",
    "    print(clean_query)\n",
    "    if course:\n",
    "        print(f\"ðŸŽ¯ Matched course (by similarity): {course}\")\n",
    "        retriever = get_filtered_retriever(course, k)\n",
    "        docs = retriever.invoke(clean_query)\n",
    "    else:\n",
    "        print(\"âš ï¸ No strong course match â€” searching across all PDFs.\")\n",
    "        retriever = get_filtered_retriever(None, k)\n",
    "        docs = retriever.invoke(clean_query)\n",
    "        \n",
    "    results = [(doc.metadata.get(\"source\", 'unknown'), doc.page_content) for doc in docs]\n",
    "    retrieval_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Retrieval time ðŸ•°ï¸ : {time.time() - start_time:.2f}s â€” {len(results)} chunks returned.\")\n",
    "    return results, retrieval_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p5wP2AmTUs7c",
    "outputId": "58d4b131-5f42-4158-8a4d-93e98f7e0f9c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 7: Test the Retriver\n",
    "\n",
    "query = \"What is described as a unique feature of the M.B.A. and Eng. in Sustainable Engineering and Project Management regarding the second year?\"\n",
    "# retriever  = retrieve(query, k=3)\n",
    "results, retrieval_time = retrieve(query, k=3)\n",
    "\n",
    "for i, (course, text) in enumerate(results, 1):\n",
    "    print(f\"\\n--- Result {i} (Course: {course} {len(text)} ---\\n{text[:900]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOFVNRnz-E-0"
   },
   "outputs": [],
   "source": [
    "# Step 8: Create a prompt template\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert assistant. Answer the question using ONLY the provided context.\n",
    "If the answer is not in the context, say you don't know.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205,
     "referenced_widgets": [
      "b488ddbd1a5f4947a55ed5a75c3df854",
      "9a946fec4f5b4092bd18472f4f7785db",
      "33e1281124ca4dfcb30dc01155652811",
      "b95ec262090e47b08c381e4ad38fa0a6",
      "a0b13b4903da44c2958f35e1686d496d",
      "0045cd00b59940b1b6e42e96ca831898",
      "2a48074ef66749f4b60df993213c422b",
      "cdb18dc07d5c42ed854031221573fb1a",
      "a827fec35e5347d3a9f1bd2676bb6a43",
      "1ba2cf31f1e64fbcbadcc673726c2792",
      "185efa9ef2f745a4a53a3a5ebea02152"
     ]
    },
    "id": "_JFNtL6y-aFk",
    "outputId": "25efb268-edc3-4697-f017-d82f8a9381c9"
   },
   "outputs": [],
   "source": [
    "# Step 9: Define LLM\n",
    "\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "\n",
    "# 1. model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model_id = \"mistralai/Ministral-8B-Instruct-2410\"\n",
    "# 3. model_id = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map = \"auto\",\n",
    "    dtype = \"auto\",\n",
    ")\n",
    "\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=llm_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=2048,\n",
    ")\n",
    "\n",
    "# Create HuggingFacePipeline instance\n",
    "hf_llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "# Pass the HuggingFacePipeline instance as the 'llm' argument to ChatHuggingFace\n",
    "llm = ChatHuggingFace(llm=hf_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xKZpq0AT9boo",
    "outputId": "ad095d8d-1d52-4c39-d96d-3756c173bdcd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test prompt to check if llm response is received\n",
    "test_prompt = PromptTemplate.from_template(\"Say hello.\")\n",
    "print(llm.invoke(test_prompt.invoke({})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lpn0naJ2Pko5"
   },
   "outputs": [],
   "source": [
    "# Step 10: Build a langchain\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "rag_chain = (RunnableParallel({\n",
    "    \"context\" : lambda x: retrieve(x[\"question\"], k=3),\n",
    "    \"question\" : lambda x: x[\"question\"]\n",
    "}))|prompt|llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZv9dXyJQcHN",
    "outputId": "1ed68ed8-390e-447a-c633-55faf4213ba2"
   },
   "outputs": [],
   "source": [
    "# Step 11: Test the chain\n",
    "response = rag_chain.invoke({\"question\": \"Give me Program objectives for ai in supply chain?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_relevant(chunk_text, ground_truth, threshold=0.6):\n",
    "    \"\"\"\n",
    "    Returns True if retrieved chunk is semantically similar\n",
    "    to the ground-truth answer.\n",
    "    \"\"\"\n",
    "    emb_chunk = model.embed_documents([chunk_text])[0]\n",
    "    emb_gt = model.embed_documents([ground_truth])[0]\n",
    "    score = util.cos_sim(emb_chunk, emb_gt).item()\n",
    "    return score >= threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k_semantic(retrieved_texts, ground_truth, k, threshold=0.6):\n",
    "    top_k = retrieved_texts[:k]\n",
    "    for text in top_k:\n",
    "        if is_relevant(text, ground_truth, threshold):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def mrr_semantic(retrieved_texts, ground_truth, threshold=0.6):\n",
    "    for rank, text in enumerate(retrieved_texts, start=1):\n",
    "        if is_relevant(text, ground_truth, threshold):\n",
    "            return 1.0 / rank\n",
    "    return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "with open(\"raga_eval_dataset_english.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "records = []\n",
    "\n",
    "for sample in tqdm(eval_data):\n",
    "    query = sample[\"query\"]\n",
    "    ground_truth = sample[\"ground_truth\"]\n",
    "\n",
    "    results, retrieval_time = retrieve(query, k=5)\n",
    "\n",
    "    retrieved_texts = [text for _, text in results]\n",
    "    mrr = mrr_semantic(retrieved_texts, ground_truth)\n",
    "\n",
    "    records.append({\n",
    "        \"query\": query,\n",
    "        \"MRR\": mrr,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(records)\n",
    "\n",
    "Mean_Recall_Rate = {\n",
    "    \"MRR\": df[\"MRR\"].mean(),\n",
    "}\n",
    "\n",
    "print(Mean_Recall_Rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Import the RAGAS\n",
    "\n",
    "from ragas import evaluate\n",
    "import json\n",
    "from ragas.metrics import (\n",
    "    ContextPrecision,\n",
    "    ContextRecall,\n",
    "    Faithfulness,\n",
    "    ResponseGroundedness,\n",
    "    AnswerAccuracy,\n",
    "    SemanticSimilarity\n",
    ")\n",
    "\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13: Define function to execute the query without using langchain\n",
    "def run_rag(question, k=3):\n",
    "    results, retrieval_time = retrieve(question, k)\n",
    "    contexts = [text for _, text in results]\n",
    "\n",
    "    prompt_text = prompt.format(\n",
    "        question=question,\n",
    "        context=\"\\n\\n\".join(contexts)\n",
    "    )\n",
    "\n",
    "    llm_start = time.time()\n",
    "    response = llm.invoke(prompt_text)\n",
    "    llm_time = time.time() - llm_start\n",
    "\n",
    "    return {\n",
    "        \"answer\": response.content,\n",
    "        \"contexts\": contexts,\n",
    "        \"retrieval_time\": retrieval_time,\n",
    "        \"llm_time\": llm_time,\n",
    "        \"total_time\": retrieval_time + llm_time\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 14a: Import the Evaluation data and evaluate it\n",
    "\n",
    "with open(\"raga_eval_dataset_english.json\", \"r\") as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "ragas_records = []\n",
    "total_retrieval_time = 0\n",
    "total_generation_time = 0\n",
    "\n",
    "for item in eval_data:\n",
    "    result = run_rag(item[\"query\"], k=3)\n",
    "    total_retrieval_time = total_retrieval_time + result[\"retrieval_time\"]\n",
    "    total_generation_time = total_generation_time + result[\"llm_time\"]\n",
    "\n",
    "    answer = result[\"answer\"]\n",
    "    contexts = result[\"contexts\"]\n",
    "\n",
    "    ragas_records.append({\n",
    "        \"question\": item[\"query\"],\n",
    "        \"answer\": answer,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truth\": item[\"ground_truth\"]\n",
    "    })\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "ragas_dataset = Dataset.from_list(ragas_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 14b: Import the Evaluation data and evaluate it\n",
    "\n",
    "with open(\"raga_eval_dataset_english.json\", \"r\") as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "ragas_records_sematic = []\n",
    "\n",
    "for item in eval_data:\n",
    "    result = run_rag(item[\"query\"], k=3)\n",
    "    \n",
    "    answer = result[\"answer\"]\n",
    "    contexts = result[\"contexts\"]\n",
    "\n",
    "    ragas_records_sematic.append({\n",
    "        \"question\": item[\"query\"],\n",
    "        \"answer\": answer,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truth\": item[\"ground_truth\"]\n",
    "    })\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "ragas_records_sematic = Dataset.from_list(ragas_records_sematic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#  Enter OpenAI api key here\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"****\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 15a: Evaluate the RAG\n",
    "\n",
    "from ragas.run_config import RunConfig\n",
    "results1 = evaluate(\n",
    "    dataset=ragas_dataset,\n",
    "    metrics=[\n",
    "        ContextPrecision(),\n",
    "        ContextRecall(),\n",
    "        Faithfulness(),\n",
    "    ],\n",
    "    embeddings=model,\n",
    "    run_config=RunConfig(max_workers=1)\n",
    ")\n",
    "\n",
    "print(results1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 15b: Evaluate the RAG\n",
    "\n",
    "results2 = evaluate(\n",
    "    dataset=ragas_records_sematic,\n",
    "    metrics=[\n",
    "        SemanticSimilarity()\n",
    "    ],\n",
    "    embeddings=model,\n",
    "    llm = llm,\n",
    "    run_config=RunConfig(max_workers=1)\n",
    ")\n",
    "\n",
    "print(results2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"For Embedding model:{embedding_model},  LLM: {model_id}, Vector Store: FAISS \") \n",
    "print(f\"Total Retrieval Time: {total_retrieval_time}\")\n",
    "print(f\"Total Generation Time per query: {total_generation_time}\")\n",
    "print(f\"Total Indexing Time: {total_indexing_time}\")\n",
    "print(f\"Mean Recall Rate: {Mean_Recall_Rate.values()}\")\n",
    "print(\"RAGAS metrics\")\n",
    "print(results1)\n",
    "print(results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_folder_size(path):\n",
    "    total = 0\n",
    "    for dirpath, _, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total += os.path.getsize(fp)\n",
    "    return total\n",
    "\n",
    "size_bytes = get_folder_size(\"faiss_course_db\")\n",
    "print(f\"Size: {size_bytes / (1024**2)} MB\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0045cd00b59940b1b6e42e96ca831898": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "185efa9ef2f745a4a53a3a5ebea02152": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1ba2cf31f1e64fbcbadcc673726c2792": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a48074ef66749f4b60df993213c422b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "33e1281124ca4dfcb30dc01155652811": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cdb18dc07d5c42ed854031221573fb1a",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a827fec35e5347d3a9f1bd2676bb6a43",
      "value": 2
     }
    },
    "9a946fec4f5b4092bd18472f4f7785db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0045cd00b59940b1b6e42e96ca831898",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2a48074ef66749f4b60df993213c422b",
      "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
     }
    },
    "a0b13b4903da44c2958f35e1686d496d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a827fec35e5347d3a9f1bd2676bb6a43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b488ddbd1a5f4947a55ed5a75c3df854": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9a946fec4f5b4092bd18472f4f7785db",
       "IPY_MODEL_33e1281124ca4dfcb30dc01155652811",
       "IPY_MODEL_b95ec262090e47b08c381e4ad38fa0a6"
      ],
      "layout": "IPY_MODEL_a0b13b4903da44c2958f35e1686d496d"
     }
    },
    "b95ec262090e47b08c381e4ad38fa0a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ba2cf31f1e64fbcbadcc673726c2792",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_185efa9ef2f745a4a53a3a5ebea02152",
      "value": "â€‡2/2â€‡[00:26&lt;00:00,â€‡13.01s/it]"
     }
    },
    "cdb18dc07d5c42ed854031221573fb1a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
